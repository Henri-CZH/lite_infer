工作年限
工作内容
技术背景
深知大模型推理加速的痛点:
长上下文，请求长度不均匀，针对这个背景，做了一些调研:
1. flash attention
2. KV cache
3. 

4. 腾讯依恋推理框架



请介绍Transformer:
1. 诞生背景->what->why
2. 架构的->how->encode only, decode only, encode+decode
3. preLayerNorm和postLayerNorm的区别
4. prefill和decode的区别: 计算密集型和访存密集型和带宽密集型, PD分离->prefill 计算能力强的GPU, decode大显存的GPU
5. 计算密集型和访存密集: GQA, MLA, 访存密集: KV cache
6. 大模型显存占用大小: 模型权重+KV cache
7. 大模型量化的指标: 模型精度
8. flash attention: 算子融合
9. 什么算子放在CPU比较好，什么放在GPU比较好
10. 分布式推理
11. LM cache?